<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>FIC Kubernetes</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<style>
			.reveal pre {
				box-shadow: none;
			}

			.reveal pre code {
				box-shadow: 0px 0px 6px rgba(0, 0, 0, 0.3);
			}
		</style>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>FIC Kubernetes</h1>
					<p>
						December 4-5-6<sup>th</sup> 2019 
					</p>
				</section>
				<section>
					<section>
						<h1>About Us</h1>
					</section>
					<section>
						<h2>Axel PEREIRA</h1>
					</section>
					<section>
						<h2>Joani COUMA</h2>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter I</h1>
						<h3>Docker & Containerisation</h3>
					</section>
					<section>
						<h2>Containerisation</h2>
						<h3>Problems</h3>
						<ul>
							<li>Environment reproducibility</li>
							<li>Complexity to deploy</li>
							<li>OS security patch</li>
						</ul>
					</section>
					<section>
						<h2>Containerisation</h2>
						<h3>Solution</h3>
						<ul>
							<li>Container isolation</li>
							<li>Lightweight</li>
							<li>Secure</li>
						</ul>
					</section>
					<section>
						<h2>Docker</h2>
						<ul>
							<li>Build an image</li>
							<li>Run a container</li>
							<li>Composed of layers</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter II</h1>
						<h3>Context</h3>
					</section>
					<section>
						<h1>Kubernetes Kesako ?</h1>
						<ul>
							<li>Docker</li>
							<li>Orchestrator</li>
							<li>Container</li>
						</ul>
					</section>
					<section>
						<h1>Kubernetes 101</h1>
						<ul>
							<li>Orchestrating Docker Images</li>
							<li>Specificaly designed to simplify the life cycle of containers</li>
							<li><i>It knows</i> where to put your containers, and how much CPU / RAM to give.</li>
						</ul>
					</section>
					<section>
						<h1>Kubernetes 102</h1>
						<ul>
							<li>Failover management</li>
							<li>HPA Capabilities (multi node setup)</li>
							<li>Load Balancing</li>
							<li>Inter container communication</li>
						</ul>
					</section>	
					<section>
						<h1>Kubernetes 103</h1>
						<ul>
							<li>Distributed / Persistent Storage</li>
							<li>Secret & Configuration management</li>
							<li>Scalable deployments</li>
							<li>cli & api management</li>
							<li>Full descriptive yml language</li>
						</ul>
					</section>	
				</section>
				<section>
					<section>
						<h1>Chapter III</h1>
						<h3>Kubernetes Dive in</h3>
					</section>
					<section>
						<h2>Minikube</h2>
						<ul>
							<li>Local Kubernetes Cluster</li>
							<li>Single Node Setup (Master / Node)</li>
						</ul>
					</section>
					<section>
						<h2>Kubernetes Resources</h2>
						<ul>
							<li>Pods</li>
							<li>ReplicaSets</li>
							<li>Deployment</li>
							<li>Services</li>
							<li>Ingress</li>
							<li>Volumes</li>
							<li>Configuration & Secrets</li>
							<li>StatefullSets</li>
							<li>Limit, Quotas, HPA</li>
							<li>...</li>
						</ul>
					</section>
					<section>
						<h2>Kubernetes Dashboard</h2>
						<ul>
							<li>Interact with a Kubernetes cluster visually</li>
							<li>Know the state of resources </li>
							<li>Access logs</li>
						</ul>
					</section>
					<section>
						<h2>Kubernetes Dashboard</h2>
							<pre>
								<code data-trim class="bash">
									minikube dashboard -url
								</code>
								<code data-trim class="bash">
									kubectl proxy
								</code>
							</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<ul>
							<li>Main way to interact with a Kubernetes Cluster</li>
							<li>Everything that can be done inside the Dashboard can be done through <b>kubectl</b></li>
						</ul>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Level 1</h4>
						<pre>
							<code data-trim class="bash">
								kubectl create 	# Create a resource from stdin or file
								kubectl run 	# Run a particular Image
								kubectl set 	# Set specific features on objects
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Level 2 (Resource manipulation)</h4>
						<pre>
							<code data-trim class="bash">
								kubectl get 		# Display one or many resources
								kubectl explain 	# Documentation of resources
								kubectl edit 		# Set specific features on objects
								kubectl delete  	# Delete resources by filename, resources / names
								kubectl apply 		# Apply a new configuration to a ressource
								kubectl label 		# Update a label on a resource
								kubectl annotate 	# Update annotation on resources 
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Level 2 (Debug and Diagnostics)</h4>
						<pre>
							<code data-trim class="bash">
								kubectl describe  	# Show details of a specific resource
								kubectl logs 		# Print logs for a pod or container
								kubectl attach		# Attach to a running container
								kubectl exec		# Execute command in a container
								kubectl port-forward	# Forward one or more locals ports to a pod
								kubectl proxy		# Run a proxy to the Kubernetes API Server
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Level 3 (Deployments)</h4>
						<pre>
							<code data-trim class="bash">
								kubectl rollout 	# Manage rollout of the resource
								kubectl scale 		# Set a new size for a deployment
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Level 4 (Administration)</h4>
						<pre>
							<code data-trim class="bash">
								kubectl cluster-info 	# Display cluster informations
								kubectl top 		# Display Resources usages
								kubectl drain		# Drain nodes in preparation for maintenance
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli kubectl</h2>
						<h4>Other commands</h4>
						<pre>
							<code data-trim class="bash">
								kubectl config 	# Update kubeconfig file
								kubectl help 	# Help about any command
								kubectl version # Print the client and server version informations
							</code>
						</pre>
					</section>
					<section>
						<h2>Kubernetes Cli Hands-On</h2>
					</section>
					<section>
						<p>
							<em><b>Run</b> a new <b>nginx:latest</b> pod and <b>expose port 80</b> and <b>get</b> its informations on the command line</em>
						</p>
					</section>
					<section>
						<pre>
							<code data-trim class="bash">
								kubectl run nginx --image=nginx:latest --port=80
								kubectl get pods
								kubectl get pods -o wide
								kubectl describe pods nginx
							</code>
						</pre>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter IV</h1>
						<h3>Pods & Organisation</h3>
					</section>
					<section>
						<ul>
							<li>Pods Concept & Model</li>
							<li>Yaml Descriptors</li>
							<li>Labels & Annotations</li>
							<li>Namespaces</li>
							<li>Pods lifecycles</li>
						</ul>
					</section>
					<section>
						<h2>Pods</h2>
					</section>
					<section>
						<ul>
							<li>One or many containers contained inside one pod</li>
							<li>Everything is based around a pod</li>
							<li>Containers inside a pod share the same network</li>
							<li>Containers inside a pod share the same volumes </li>
						</ul>
					</section>
					<section>
						<h2>Yaml descriptors</h2>
					</section>
					<section>
						<h2>Nginx pod description</h2>
						<pre>
							<code data-trim class="yml">
								apiVersion: v1
								kind: Pod
								metadata:
								  name: nginx
								spec:
								  containers:
								    - name: nginx
								      image: nginx:latest
								      imagePullPolicy: IfNotPresent
								      ports:
								      - containerPort: 80
							</code>
						</pre>
					</section>
					<section>
						<h2>Describing the spec with explain</h2>
						<pre>
							<code data-trim class="bash">
								kubectl explain pods
							</code>
						</pre>
					</section>
					<section>
						<h2>Creating a resource way</h2>
						<pre>
							<code data-trim class="bash">
								kubectl create -f pod-file.yml
								kubectl apply -f pod-file.yml
							</code>
						</pre>
					</section>
					<section>
						<p>
							Multiple resources can be described in a single file by separating them by <b>---</b> (Three hyphens)
						</p>
					</section>
					<section>
						<h2>Organizing pods with labels (1/3)</h2>
						<ul>
							<li>In real life scenarios you will be working with more than one pod</li>
							<li>Microservices, multi-environment, replication</li>
						</ul>
					</section>
					<section>
						<h2>Organizing pods with labels (2/3)</h2>
						<ul>
							<li>Organize subgroups of pods</li>
							<li>Add informations to pods / services</li>
							<li>key/value pairs, keys must be unique !</li>
						</ul>
					</section>
					<section>
						<h2>Organizing pods with labels (3/3)</h2>
						<ul>
							<li>Keys should be of the form <em>dns.prefix/name</em></li>
							<li>dns prefix are optional</li>
							<li>Values are any alphanumeric, 63 chars long, strings and can contain hyphens, underscores, points.</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    cgi.com/my-label: foo
    app: nginx
    env: dev
spec:
  containers:
  - name: nginx
    image: nginx:latest
	imagePullPolicy: IfNotPresent
	ports:
	- containerPort: 80
</code>
						</pre>
					</section>
					<section>
						<ul>
							<li>You can put labels on pretty <em>much everything</em></li>
						</ul>
						<pre>
							<code data-trim class="bash">
								kubectl get nodes --show-labels
								kubectl get nodes --label-columns arch
							</code>
						</pre>
					</section>
					<section>
						<ul>
							<li>It is possible to constraint selected pods to selected nodes</li>
						</ul>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
  cgi.com/my-label: foo
  app: nginx
  env: dev
spec:
  nodeSelector:
    ssd: 'true'
  containers:
  - name: nginx
    image: nginx:latest
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 80
</code>
						</pre>
					</section>
					<section>
						<h2>Annotations (1/2)</h2>
						<ul>
							<li>Unlike labels, annotations are not designed to select specific resources</li>
							<li>They are used to contains informations like Build number, pull request number, contact address...</li>
						</ul>
					</section>
					<section>
						<h2>Annotations (2/2)</h2>
						<pre>
<code data-trim class="bash">
kubectl annotate pod nginx cgi.com/created-by=coumaj

kubectl describe nginx | grep Annotations
</code>
						</pre>
					</section>
					<section>
						<h2>Namespaces</h2>
						<ul>
							<li>To further more organize your clusters, you can (should) use namespaces</li>
							<li>They are virtual clusters</li>
							<li>They allow the grouping of multiples resources (of any types)</li>
							<li>A resource name is unique inside a namespace (not a cluster)</li>
							<li>Resources quotas can be setup on namespaces</li>
						</ul>
					</section>
					<section>
						<h2>Namespaces</h2>
						<ul>
							<li>By default, you are using the default namespace</li>
							<li>Namespaces can be created at any time, and every command you send to the cluster is sent against your current namespace</li>
							<li>You can add the option --namespace=namespace-name to any command to target a specific namespace</li>
						</ul>
					</section>
					<section>
						<h2>Namespaces</h2>
						<pre>
							<code data-trim class="bash">
								kubectl config current-context # Show the current cluster
								kubectl config set-context current-context --namespace=target-namespace
								# The last command changes the current namespace
							</code>
						</pre>
					</section>
					<section>
						<h2>Namespaces</h2>
						<pre>
							<code data-trim class="bash">
								kubectl get ns
								kubectl describe ns 
								kubectl create ns 
								kubectl create -f k8s-ns.yml 
							</code>
						</pre>
					</section>
					<section>
						<h2>Namespaces</h2>
						<ul>
							<li>You can add in the metadata spec a key/value namespace=namespaceName</li>
							<li>Or add it when using the command create / apply</li>
						</ul>
						<pre>
							<code data-trim class="bash">
								kubectl create -f file.yml -n namespace 
								# Create a resource inside a specific namespace
							</code>
						</pre>
					</section>
					<section>
						<h2>Namespaces</h2>
						<ul>
							<li>When deleting a namespace, you delete every resources inside it !</li>
							<li>You can't delete default namespaces (kube-system, default, kube-public)</li>
						</ul>
					</section>
					<section>
						<h2>Pod lifecycle</h2>
						<ul>
							<li>When a pod dies, it can't be resurected</li>
							<li>When a container inside a pod dies, depending on the restart policy, it can be automatically restarted</li>
						</ul>
					</section>
					<section>
						<h2>Pods lifecycle</h2>
						<ul>
							<li>You can see the logs of a pod by using the command <b>kubectl logs</b></li>
							<li>Or to see the logs of a specific container <b>kubectl logs podName -c containerName</b></li>
						</ul>
					</section>
					<section>
						<h2>Init container</h2>
						<ul>
							<li>One or more init container</li>
							<li>Sequential execution</li>
							<li>It must succeed</li>
							<li>Used to initialize or delay a container</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter V</h1>
						<h3>Pod Priority & Health-probes</h3>
					</section>
					<section>
						<h2>Pod priority</h2>
						<ul>
							<li>Protect critical pods</li>
							<li>Multi env priority</li>
							<li>Soft preemption policy</li>
						</ul>
					</section>
					<section>
						<h2>Health-probes</h2>
						<h3>Problems</h3>
						<ul>
							<li>Slow application startup</li>
							<li>Incoming request before pod starts</li>
							<li>Auto fail recovering</li>
						</ul>
					</section>
					<section>
						<h2>Health-probes</h2>
						<h3>Startup probes</h3>
						<ul>
							<li>Succeed or die</li>
							<li>Protect slow starting containers</li>
						</ul>
					</section>
					<section>
						<h2>Health-probes</h2>
						<h3>Liveness probes</h3>
						<ul>
							<li>Restart stucked application</li>
						</ul>
					</section>
					<section>
						<h2>Health-probes</h2>
						<h3>Readiness</h3>
						<ul>
							<li>Allow routing on success</li>
							<li>Disable routing on fail</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter VI</h1>
						<h3>Services</h3>
					</section>
					<section>
						<ul>
							<li>Services</li>
							<li>Exposing internal Services</li>
							<li>Exposing exteral Services</li>
							<li>Ingresses</li>
						</ul>
					</section>
					<section>
						<ul>
							<li>Pods are ephemeral</li>
							<li>Kubernetes assign each pods a new IP Address just before it's created</li>
							<li>Client application should not have to know each pod address</li>
							<li>That's why <b>Services</b> are usefull</li>
						</ul>
					</section>
					<section>
						<ul>
							<li>Services are created to give a single entry point to one or more pods</li>
							<li>Each service obtain an IP Adresse which doesn't change until it's deleted</li>
							<li>Client application should target theses IP addresses along with a port number</li>
							<li>Each pod can live / die behind a service has it's IP doesn't have to be known</li>
						</ul>
					</section>
					<section>
						<ul>
							<li>Each Service has a type</li>
							<li>By default a Service is of type ClusterIP</li>
							<li>The IP Address of the service is internal to the cluster</li> 
							<li>It's a virtual IP, it can't be pinged !</li>
							<li>Services can expose multiple ports !</li>
						</ul>
					</section>
					<section>
						<p>A typical Spring Boot app exposing main port & admin port</p>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Service
metadata:
  name: svc-springboot
spec:
  selector:
    app: svc-springboot
  ports:
  - port: 80
    targetPort: 8080
  - port: 8000
    targetPort: 8081
</code>
						</pre>
					</section>
					<section>
						<ul>
							<li>You can even name the ports !</li>
							<li>This way, resources using these ports will not be impacted if they change</li>
						</ul>
					</section>
					<section>
<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Service
metadata:
  name: svc-springboot
spec:
  selector:
	app: svc-springboot
  ports:
	- port: 80
      name: web
	  targetPort: 8080
	- port: 8000
      name: admin
      targetPort: 8081
</code>
						</pre>
					</section>
					<section>
						<h2>Service discovery</h2>
						<ul>
							<li>ClusterIP is not an easy way to talk with the services</li>
							<li>Each services can be accessed though the internal Kube DNS via their DNS address</li>
							<li><b>serviceName.namespace.svc.cluster.local</b></li>
							<li>Inside the same namespace, each service can be accessed only though their name</li>
						</ul>
					</section>
					<section>
						<h2>Accessing Services from outside the Kube cluster</h2>
					</section>
					<section>
						<p>If you want to access any service from outside, you must change it's type</p>
						<ul>
							<li>NodePort</li>
							<li>LoadBalancer</li>
						</ul>
						<p>Or use an Ingress Controller (more on that later)</p>
					</section>
					<section>
						<h2>NodePort Service</h2>
						<ul>
							<li>Kubernetes will assign the same port to each of it's node</li>
							<li>For internal communication a ClusterIP is created</li>
						</ul>
					</section>
					<section>
						<pre>			
<code data-trim class="yml">
apiVersion: v1
kind: Service
metadata:
  name: whoami-nodeport
spec:
  type: NodePort
  selector:
    app: whoami
  ports
  - port: 8080 
    targetPort: 80 
    nodePort: 30123
</code>
			</pre>
					</section>
						<section>
							<ul>
								<li>If not port is specified, Kubernetes will assign one (30000-32767 by default)</li>
							</ul>
						</section>
						<section>
							<h2>Load Balancer Service</h2>
							<ul>
								<li>Some Cloud providers provide Load Balancing technology</li>
								<li>When creating a service of this type, Kube will call the CP API to provision / configure the Cloud Service</li>
								<li>When the load balancer is created, the IP address of this load balancer will be assigned to the service</li>
								<li>Technically, this service uses NodePort to allow the external load balancer to communicate to the Cluster</li>
							</ul>
						</section>
						<section>
							<h2>Ingress</h2>
						</section>
						<section>
							<h2>Ingress</h2>
							<ul>
								<li>All the Kubernetes cluster are configured to work with external Load Balancers</li>
								<li>NodePort is really not the easy way (and dirty)</li>
								<li>Services works at the TCP layer. So no cookie affinity if working with HTTPs</li>
								<li><b>Ingresses</b> are !</li>
							</ul>
						</section>
						<section>
							<h2>Ingress</h2>
							<ul>
								<li>To use the ingress ressource, you will have to setup an Ingress Controller</li>
								<li>Multiple Ingress Controllers exists (nginx, Traefik, Haproxy...)</li>
							</ul>
						</section>
				</section>
				<section>
					<section>
						<h1>Chapter VII</h1>
						<h3>Deployments</h3>
					</section>
					<section>
						<h2>Deployments</h2>
						<ul>
							<li>When working with pods, you will have to configure each individual pods, labelling them to be used by specific Services</li>
							<li>Rolling updates are pretty painfull</li>
							<li>Scalability is a no go when working only with pods</li>
						</ul>
					</section>
					<section>
<pre>
<code data-trim class="yml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia-node
        image: luksa/kubia:v1
</code>
</pre>
					</section>
					<section>
						<h2>Deployments</h2>
						<p>
							<em>Apply</em> this file and check the <em>rollout</em> of the <em>deployment</em> 
						</p>
					</section>
					<section>
						<pre>
							<code data-trim class="bash">
								kubectl apply -f kubia.yml
								kubectl rollout status deployment kubia
								kubectl rollout history deployment kubia 
								kubectl rollout undo deployment kubia --to-revision=1
							</code>
						</pre>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter VIII</h1>
						<h3>Volumes</h3>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>Files created inside a pod are ephemerals</li>
							<li>When a pod is recreated, any new container will not have access to old container files</li>
							<li>Multi-container pods might want to share files beetween them</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>Many types of volumes exists, we will focus on the main used </li>
							<li>emptyDir : initialy empty dir</li>
							<li>persistenVolumeClaim : dynamically allocated volumes</li>
							<li>Volumes exposed by cloud providers (azureDisk for example)</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>emptyDir might not be as usefull as others, but it allows multiple containers inside a pod to share files</li>
							<li>It's lifetime is linked to the pod lifetime</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: apps/v1
kind: Deplyment
metadata:
  name: kubia
spec:
  containers:
    - image ….
      volumeMounts:
      - mountPath: /var/log/nginx
        name: var-log-nginx
    - image: ….
      volumeMounts:
      - mountPath: /data
        name: var-log-nginx
  volumes:
  - name: var-log-nginx
    emptyDir: {}
</code>
						</pre>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>Managing disk space has to be taken from a specific approach</li>
							<li>Using PersistentVolumes & PersistentVolumesClaims allows cluster admins to give an abstract layer to the volume management</li>
							<li><b>PersistentVolume</b> is a storage reserved and configured by cluster admins</li>
							<li><b>PersistentVolumesClaim</b> are storage requests the users can make to the PV</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>PersistentVolumes can be Static or Dynamic</li>
							<li>Static PV are essentially direct storage access and persistent</li>
							<li>Dynamic PV are PV created by the cluster</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>PVC are created by designing a wanted storage size</li>
							<li>The cluster choose the correct PV corresponding to the size needed</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<ul>
							<li>3 access modes exists to get access to storage</li>
							<li><b>ReadWriteOnce</b> : Only one node can mount the volume with read-write access</li>
							<li><b>ReadOnlyMany</b> : Multiple node can mount the volume with read-only access</li>
							<li><b>ReadWriteMany</b> : Multiple node can mount the volume with read-write access</li>
						</ul>
					</section>
					<section>
						<h2>Volumes</h2>
						<h3>Volume Claim Template</h3>
						<ul>
							<li>Managed by the ressource</li>
							<li>Automatically create a PV</li>
							<li>Can be configured with StorageClass</li>
						</ul>
					</section>
				</section>
				<!-- This section is for the second day -->
				<section>
					<section>
						<h1>Chapter IX</h1>
						<h3>Previously on day one...</h3>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>Pod</h3>
						<ul>
							<li>Labels</li>
							<li>Containers</li>
							<li>The central ressource</li>
						</ul>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>Deployment</h3>
						<ul>
							<li>Label selector</li>
							<li>Pod template</li>
							<li>ReplicaSet</li>
							<li>Env variables</li>
						</ul>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>Service</h3>
						<ul>
							<li>Label selector</li>
							<li>Port, TargetPort</li>
							<li>ClusterIP & NodePort</li>
							<li>Internal kubernetes DNS</li>
						</ul>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>Ingress</h3>
						<ul>
							<li>Annotations</li>
							<li>Paths & backend rule</li>
						</ul>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>StatefulSets</h3>
						<ul>
							<li>Deployment like</li>
							<li>Designed for stateful app</li>
						</ul>
					</section>
					<section>
						<h2>Resources</h2>
						<h3>Volumes</h3>
						<ul>
							<li>PVC & PV</li>
							<li>EmptyDir & managed storage</li>
							<li>StorageClass</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter X</h1>
						<h3>Configuration & Secrets</h3>
					</section>
					<section>
						<h2>Configuration & Secrets</h2>
						<ul>
							<li>Docker philosophie goes towards having the exact same image beetween environments</li>
							<li>Each environments has it's own specificities</li>
							<li>It's common to use environment variables to store JDBC urls, spring profiles etc</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
    - name: mysql-pod
      image: mysql:5.7.24
      imagePullPolicy: IfNotPresent
      env:
       - name: MYSQL_ROOT_PASSWORD
       value: root
       - name: MYSQL_USER
       value: admin
       - name: MYSQL_URL
       value: $(MYSQL_USER):$(MYSQL_ROOT_PASSWORD)@localhost:1521
</code>
						</pre>
					</section>
					<section>
						<h2>Configuration & Secrets</h2>
						<ul>
							<li>But... Using environment variables has it's problems</li>
							<li>Each environment have to have it's own descriptor</li>
							<li>Descriptors are Source Code and thus should not be environment dependant</li>
							<li><b>ConfigMap</b> to the rescue !</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-map
data:
  app.version: 1.0.0
  app.context-liquibase: develop
  app.sha1: d7d9b5cd7a9aa501301f572477982315ded06c29
</code>
						</pre>
					</section>
					<section>
						<h2>Configuration & Secrets</h2>
						<ul>
							<li>ConfigMap can be created via descriptors</li>
							<li>Keys must be valid environment variables</li>
							<li>ConfigMap must be created before referencing it to a Pod </li>
							<li>If using a Pod is using a key which is not in a ConfigMap the Pod will not start</li>
							<li>ConfigMap are valid inside they own namespace</li>
						</ul>
					</section>
					<section>
						<h2>Configuration & Secrets</h2>
						<ul>
							<li>Sometimes, some informations are to important to be left in a ConfigMap</li>
							<li>You will want to store sensible information like credentials, SSH keys, certificates...</li>
							<li><b>Secrets</b> is the answer</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Secret
metadata:
  name: mysql-password-secret
type: Opaque
data:
  db.password: VGgxczFzUkBAdCE=
</code>
						</pre>
					</section>
					<section>
						<h2>Configuration & Secrets</h2>
						<ul>
							<li>You can use Secrets & ConfigMaps just as you'd use environment variables, in fact, they just contains the values of the environments variables </li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
    - name: mysql-pod
      image: mysql:5.7.24
      imagePullPolicy: IfNotPresent
      env:
       - name: MYSQL_ROOT_PASSWORD
         valueFrom: 
           secretKeyRef:
             name: database-config-secrets
             key: db.password
       - name: MYSQL_ROOT_USER
         valueFrom:
           configMapKeyRef:
             name: database-config 
             key: db.user
</code>					
</pre>	
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter XI</h1>
						<h3>Quotas</h3>
					</section>
					<section>
						<h2>Quotas</h2>
						<ul>
							<li>When creating Pods you can express specific limits on CPU & Ram allocated to the resources</li>
							<li>Resources used by a Pod are the sum of all it's containers</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: v1
kind: Pod
…
  containers:
    - name: db
      image: mysql:5.7.24
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "64Mi"
          cpu: "250m"
</code>
						</pre>
					</section>
					<section>
						<h2>Quotas</h2>
						<ul>
							<li>A container can use more than it's requested memory if the node has enough</li>
							<li>A container can't use more than it's limit memory</li>
							<li>It could automatically be deleted by Kubernetes if it exceeds the limit value</li>
							<li>A container can use more than it's requested CPU if the node has enough</li>
							<li>A container can't use more thant it's limit CPU</li>
						</ul>
					</section>
					<section>
						<h2>Quotas</h2>
						<ul>
							<li>Kubernetes will choose by itself on wich node to setup a pod based on the requested CPU/Ram values</li>
							<li>It is not the used memory / cpu in a specific point in time, but the sum of the requested values</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter XII</h1>
						<h3>Helm</h3>
					</section>
					<section>
						<h3>Helm</h3>
						<ul>
							<li>Templating and dependency management of K8S resources</li>
							<li>All-in-one Stacks installations</li>
							<li>Clean & reusable file tree</li>
							<li>Helm VS Kustomize</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter XIII</h1>
						<h3>Previously on Day 2</h3>
					</section>
					<section>
						<h3>Configuration & Secrets</h3>
						<ul>
							<li>ConfigMap</li>
							<li>Secrets</li>
							<li>https://www.base64encode.org/</li>
						</ul>
					</section>
					<section>
						<h3>Quotas</h3>
						<ul>
							<li>QoS Class</li>
							<li>Request / Limit on CPU / RAM</li>
						</ul>
					</section>
					<section>
						<h3>Health Probes</h3>
						<ul>
							<li>Readiness probes</li>
							<li>Liveness probes</li>
							<li>Startup propbes</li>
						</ul>
					</section>
					<section>
						<h3>Pod Priority</h3>
						<ul>
							<li>Priority level</li>
							<li>Only on install</li>
						</ul>
					</section>
					<section>
						<h3>K9s</h3>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter XIV</h1>
						<h3>DaemonSet</h3>
					</section>
					<section>
						<ul>
							<li>ReplicaSets (used by Deployments & StatefulSet) run any number of pods anywhere on the cluster</li>
							<li>We might want to run one and only one pod on each nodes</li>
							<li><b>DaemonSet !</b></li>
						</ul>
					</section>
					<section>
						<h3>DaemonSets</h3>
						<ul>
							<li>Logs collecting agent (fluentd, logstash, ... )</li>
							<li>Metrics collecting agent (prometheus, zabbix, ... )</li>
						</ul>
					</section>
					<section>
						<h3>DaemonSets</h3>
						<ul>
							<li>If a new node is spawned, the DaemonSet will listen for that & create a new pod on the new node</li>
							<li>You can use nodeSelector to select specific nodes to deploy pods from a DaemonSet</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: europe-west-monitor
spec:
  selector:
    matchLabels:
	  zone: europe-west
  template:
    metadata:
      labels:
        app: europe-west
    spec:
      nodeSelector:
        zone: europe
        sub-zone: west
      containers:
      - name: main
        image: ….
</code>
						</pre>
					</section>
				</section>
				<section>
					<section>
						<h1>Chapter XV</h1>
						<h3>Job & CronJob</h3>
					</section>
					<section>
						<ul>
							<li>Pods run indefinitely until they die, restart, are stopped ... </li>
							<li>You might want to run a task until it's finishes without restarting it</li>
						</ul>
					</section>
					<section>
						<ul>
							<li>A Job will just start a pod and wait for it to finished with a success</li>
							<li>If the Pod fails, a Job might restart it</li>
						</ul>
					</section>
					<section>
						<h3>CronJobs</h3>
						<ul>
							<li>Start a process at a later time or periodically</li>
							<li>A CronJob creates Job</li>
							<li>The Job creates a Pod</li>
						</ul>
					</section>
					<section>
						<pre>
<code data-trim class="yml">
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello-cron
spec:
  schedule: "*/5 * * * *"
    jobTemplate:
      spec:
        template:
          spec:
            containers:
            - name: hello-cron
              image: busybox
              args: ["/bin/sh","-c", "date"]
            restartPolicy: OnFailure
</code>
						</pre>
					</section>
				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
